{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:13:29.794914Z","iopub.execute_input":"2024-12-30T17:13:29.795386Z","iopub.status.idle":"2024-12-30T17:13:35.018517Z","shell.execute_reply.started":"2024-12-30T17:13:29.795344Z","shell.execute_reply":"2024-12-30T17:13:35.017393Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class PatchEmbed(nn.Module):\n    def __init__(self, img_size, patch_size, in_c=3, embed_dims=768):\n        super().__init__()\n\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.n_patches = (img_size // patch_size) ** 2\n\n        self.proj = nn.Conv2d(\n            in_c,\n            embed_dims,\n            kernel_size = patch_size,\n            stride = patch_size\n        )\n\n    def forward(self, x):\n        x = self.proj(x)\n        x = x.flatten(2)\n        x = x.transpose(2, 1)\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:13:38.066173Z","iopub.execute_input":"2024-12-30T17:13:38.066698Z","iopub.status.idle":"2024-12-30T17:13:38.072686Z","shell.execute_reply.started":"2024-12-30T17:13:38.066665Z","shell.execute_reply":"2024-12-30T17:13:38.071461Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0.0, proj_p=0.0):\n        super().__init__()\n\n        self.n_heads = n_heads\n        self.dim = dim\n        self.head_dim = dim // n_heads\n        self.scale = self.head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim*3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_p)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_p)\n\n    def forward(self, x):\n        n_samples, n_tokens, dim = x.shape\n\n        if dim != self.dim:\n            raise ValueError\n\n        qkv = self.qkv(x)\n        qkv = qkv.reshape(\n            n_samples, n_tokens, 3, self.n_heads, self.head_dim\n        )\n        qkv = qkv.permute(2, 0, 3, 1, 4)\n\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        k_t = k.transpose(-2, -1)\n\n        dp = (\n            q @ k_t\n        ) * self.scale\n        \n        attn = dp.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        weighted_avg = attn @ v\n        weighted_avg = weighted_avg.transpose(\n            1, 2\n        )\n\n        weighted_avg = weighted_avg.flatten(2)\n\n        x = self.proj(weighted_avg)\n        x = self.proj_drop(x)\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:13:39.164179Z","iopub.execute_input":"2024-12-30T17:13:39.164601Z","iopub.status.idle":"2024-12-30T17:13:39.172905Z","shell.execute_reply.started":"2024-12-30T17:13:39.164558Z","shell.execute_reply":"2024-12-30T17:13:39.171724Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, in_features, hidden_features, out_features, p=0.0):\n        super().__init__()\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(p)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:13:41.711595Z","iopub.execute_input":"2024-12-30T17:13:41.711937Z","iopub.status.idle":"2024-12-30T17:13:41.717885Z","shell.execute_reply.started":"2024-12-30T17:13:41.711909Z","shell.execute_reply":"2024-12-30T17:13:41.716575Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class Block(nn.Module):\n    def __init__(self, dim, n_heads, mlp_ratio=4.0, qkv_bias=True, p=0.0, attn_p=0.0):\n        super().__init__()\n\n        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n        \n        self.attn = Attention(\n            dim,\n            n_heads=n_heads,\n            qkv_bias=qkv_bias,\n            attn_p=attn_p,\n            proj_p=p\n        )\n\n        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n        hidden_features = int(dim*mlp_ratio)\n        \n        self.mlp = MLP(\n            in_features=dim,\n            hidden_features=hidden_features,\n            out_features=dim,\n        )\n        \n    def forward(self, x):\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:13:44.373336Z","iopub.execute_input":"2024-12-30T17:13:44.373728Z","iopub.status.idle":"2024-12-30T17:13:44.380495Z","shell.execute_reply.started":"2024-12-30T17:13:44.373696Z","shell.execute_reply":"2024-12-30T17:13:44.379222Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class VisionTransformer(nn.Module):\n    def __init__(self,\n                img_size=384,\n                patch_size=16,\n                in_c=3,\n                n_classes=1000,\n                embed_dim=768,\n                depth=12,\n                n_heads=12,\n                mlp_ratio=4.0,\n                qkv_bias=True,\n                p=0.0,\n                attn_p=0.0):\n        super().__init()\n\n        self.patch_embed = PatchEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_c=in_c,\n            embed_dim=embed_dim\n        )\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(\n            torch.zeroes(1, 1 + self.patch_embed.n_patches, embed_dim)\n        )\n\n        self.pos_drop = nn.Dropout(p=p)\n\n        self.blocks = nn.ModuleList(\n            [\n                Block(\n                    dim=embed_dim,\n                    n_heads=n_heads,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    p=p,\n                    attn_p=attn_p,\n                )\n                for _ in range(depth)            ]\n        )\n\n        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n        self.head = nn.Linear(embed_dim, n_classes)\n\n\n    def forward(self, x):\n        n_samples = x.shape[0]\n        x = self.patch_embed(x)\n\n        cls_token = self.cls_token.expand(\n            n_samples, -1, -1\n        )\n        x = torch.cat((cls_token, x), dim=1)\n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        for block in self.blocks:\n            x = block(x)\n\n        x = self.norm(x)\n        cls_final_token = x[:, 0]\n        x = self.head(cls_final_token)\n\n        return x\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
