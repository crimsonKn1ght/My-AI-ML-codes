{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Any, Callable, List, Optional, Type, Union\n# from ._api import register_model, Weights, WeightsEnum","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-17T06:59:14.986975Z","iopub.execute_input":"2024-12-17T06:59:14.988235Z","iopub.status.idle":"2024-12-17T06:59:14.994405Z","shell.execute_reply.started":"2024-12-17T06:59:14.988185Z","shell.execute_reply":"2024-12-17T06:59:14.992579Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"device = torch.device('cuda')\n\nif device == 'cuda':\n    print(torch.cuda.get_device_name())\nelse:\n    print('CUDA not available')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T06:59:15.192494Z","iopub.execute_input":"2024-12-17T06:59:15.192904Z","iopub.status.idle":"2024-12-17T06:59:15.199545Z","shell.execute_reply.started":"2024-12-17T06:59:15.192870Z","shell.execute_reply":"2024-12-17T06:59:15.198375Z"}},"outputs":[{"name":"stdout","text":"CUDA not available\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"class conv3x3(nn.Module):\n    def __init__(self, in_c, out_c, kernel = 3, stride=1, padding=1):\n        super().__init__()\n\n        self.conv = nn.Conv2d(\n            in_c, out_c,\n            kernel_size = kernel,\n            stride = stride,\n            padding = padding\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass conv1x1(nn.Module):\n    def __init__(self, in_c, out_c, kernel = 1, stride=1, padding=1):\n        super().__init__()\n\n        self.conv = nn.Conv2d(\n            in_c, out_c,\n            kernel_size = kernel,\n            stride = stride,\n            apdding = padding\n        )\n\n    def forward(self, x):\n        return self.conv(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T06:59:15.377493Z","iopub.execute_input":"2024-12-17T06:59:15.377932Z","iopub.status.idle":"2024-12-17T06:59:15.385863Z","shell.execute_reply.started":"2024-12-17T06:59:15.377892Z","shell.execute_reply":"2024-12-17T06:59:15.384454Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class Bottleneck(nn.Module):\n    def __init__(self, in_c, out_c, down_sample = None, stride = 1):\n        super().__init__()\n\n        self.expansion = 4                            #what's the utility of it?\n\n        self.conv1 = conv1x1(in_c, out_c, stride = stride, padding = 0)\n        self.batch_norm1 = nn.BatchNorm2d(out_c)\n\n        self.conv2 = conv3x3(out_c, out_c, stride = stride, padding = 0)                    #stride values need to be checked\n        self.batch_norm2 = nn.BatchNorm2d(out_c)\n\n        self.conv1 = conv3x3(in_c, out_c * self.expansion, stride = stride, padding = 0)\n        self.batch_norm3 = nn.BatchNorm2d(out_c * self.expansion)\n\n        self.downsample = down_sample\n        self.stride = stride\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x_cpy = x\n        \n        x = self.conv1(x)\n        x = self.batch_norm1(x)\n        x = self.relu(x)\n\n        x = self.conv2(x)\n        x = self.batch_norm2(x)\n        x = self.relu(x)\n\n        x = self.conv3(x)\n        x = self.batch_norm3(x)\n\n        if self.downsample != None:\n            x_cpy = self.downsample(x_cpy)\n\n        x += x_cpy\n        x = self.relu(x)\n\n        return x       \n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T06:59:15.556267Z","iopub.execute_input":"2024-12-17T06:59:15.558245Z","iopub.status.idle":"2024-12-17T06:59:15.592552Z","shell.execute_reply.started":"2024-12-17T06:59:15.558156Z","shell.execute_reply":"2024-12-17T06:59:15.589282Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class BasicBlock(nn.Module):\n    def __init__(self, in_c, out_c, kernel, stride, padding, norm_layer, groups, base_width):\n        super().__init__(BasicBlock, self)\n\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n\n        if groups != 1 and base_width != 64:\n            raise ValueError('Only groups=1 & width=64 are supported.')\n\n        if dilation>1:\n            raise NotImplementedError(\"Dilation>1 can't be handled.\")\n\n        self.conv1 = conv3x3(in_c, out_c, kernel, stride, padding)\n        self.bn1 = norm_layer(out_c)\n        self.relu = nn.ReLU(inplace = True)\n        self.conv2 = conv1x1(out_c, out_c, kernel, stride, padding)\n        self.bn2 = norm_layer(out_c)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        x_cpy = x\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n\n        if self.downsample != None:\n            x_cpy = self.downsample(x_cpy)\n\n        x += x_cpy\n        x = self.relu(x)\n\n        return x\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T06:59:15.714555Z","iopub.execute_input":"2024-12-17T06:59:15.715144Z","iopub.status.idle":"2024-12-17T06:59:15.728031Z","shell.execute_reply.started":"2024-12-17T06:59:15.715086Z","shell.execute_reply":"2024-12-17T06:59:15.726805Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"class resnet34(nn.Module):\n    def __init__(self, ):\n        super().__init(resnet34, self)\n\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n\n        self._norm_layer = norm_layer\n\n        self.in_c = 64\n        self.dialation = 1\n        \n        if replace_stride_with_dilation is None:\n            replace_stride_with_dilation = [False, False, False]\n\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(\"Length of replace_stride_with_dilation isn't 3\")\n\n        self.groups = groups\n        self.base_width = width_per_group\n        self.conv1 = conv3x3(3, in_c, kernel_size=7, stride=2, padding=3, bias = False)\n        self.bn1 = norm_layer(self.in_c)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=3, bias=False)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layer[1], stride=2, dilate=replace_stride_with_dilation[0])\n        self.layer3 = self._make_layer(block, 128, layer[2], stride=2, dilate=replace_stride_with_dilation[1])\n        self.layer4 = self._make_layer(block, 128, layer[3], stride=2, dilate=replace_stride_with_dilation[2])\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode = 'fan_out', nonlinearity='relu')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, BottleNeck):\n                    nn.init.constant_(m.bn3,weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n\n    def make_layer(self, block, out_c, blocks, stride, dilate=False):\n        norm_layer = self._norm_layer\n        downsample = None\n        prev_dilation = self.dilation\n\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.in_c != out_c * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.in_c, planes * block.expansion, stride),\n                norm_layer(out_c * block.expansion)\n            )\n\n        layers = []\n        layers.append(\n            block(\n                self.in_c, out_c, stride, downsample, self.groups, self.base_width, prev_dilation, norm_layer\n            )\n        )\n        self.in_c = out_c * block.expansion\n\n        for _ in range(1, blocks):\n            layers.append(\n                block(\n                    self.in_c,\n                    out_c,\n                    groups = self.groups,\n                    base_width = self.base_width,\n                    dilation = self.dilation,\n                    norm_layer = norm_layer\n                )\n            )\n            \n        return nn.Sequential(*layers)\n\n\n    def _forward_impl(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\n\n    def forward(self, x):\n        return self._forward_impl(x)\n\ndef _resnet(\n    block: Type[Union[BasicBlock, Bottleneck]],\n    layers: List[int],\n    weights: Optional[WeightsEnum],\n    progress: bool,\n    **kwargs: Any,\n) -> ResNet:\n    if weights is not None:\n        _ovewrite_named_param(kwargs, \"num_classes\", len(weights.meta[\"categories\"]))\n\n    model = ResNet(block, layers, **kwargs)\n\n    if weights is not None:\n        model.load_state_dict(weights.get_state_dict(progress=progress, check_hash=True))\n\n    return model\n\n\n_COMMON_META = {\n    \"min_size\": (1, 1),\n    \"categories\": _IMAGENET_CATEGORIES,\n}\n\n\n# class ResNet34_Weights(WeightsEnum):\n#     IMAGENET1K_V1 = Weights(\n#         url=\"https://download.pytorch.org/models/resnet34-b627a593.pth\",\n#         transforms=partial(ImageClassification, crop_size=224),\n#         meta={\n#             **_COMMON_META,\n#             \"num_params\": 21797672,\n#             \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#resnet\",\n#             \"_metrics\": {\n#                 \"ImageNet-1K\": {\n#                     \"acc@1\": 73.314,\n#                     \"acc@5\": 91.420,\n#                 }\n#             },\n#             \"_ops\": 3.664,\n#             \"_file_size\": 83.275,\n#             \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a simple training recipe.\"\"\",\n#         },\n#     )\n#     DEFAULT = IMAGENET1K_V1\n\n\n\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T06:59:15.956412Z","iopub.execute_input":"2024-12-17T06:59:15.956931Z","iopub.status.idle":"2024-12-17T06:59:16.024279Z","shell.execute_reply.started":"2024-12-17T06:59:15.956879Z","shell.execute_reply":"2024-12-17T06:59:16.022413Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 108\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_impl(x)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_resnet\u001b[39m(\n\u001b[1;32m    106\u001b[0m     block: Type[Union[BasicBlock, Bottleneck]],\n\u001b[1;32m    107\u001b[0m     layers: List[\u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m--> 108\u001b[0m     weights: Optional[\u001b[43mWeightsEnum\u001b[49m],\n\u001b[1;32m    109\u001b[0m     progress: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    111\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResNet:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         _ovewrite_named_param(kwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(weights\u001b[38;5;241m.\u001b[39mmeta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n","\u001b[0;31mNameError\u001b[0m: name 'WeightsEnum' is not defined"],"ename":"NameError","evalue":"name 'WeightsEnum' is not defined","output_type":"error"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
